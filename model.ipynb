{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c8e9d68",
   "metadata": {},
   "source": [
    "# ü¶ô Ollama 7B/14B Models on Google Colab\n",
    "\n",
    "*   List item\n",
    "*   List item\n",
    "\n",
    "\n",
    "\n",
    "This notebook will guide you through setting up and running Ollama with either 7B or 14B parameter models on Google Colab.\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Google Colab account\n",
    "- **Required for 14B models**: GPU runtime + Colab Pro (recommended)\n",
    "  - Go to **Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU**\n",
    "\n",
    "## üöÄ What we'll accomplish:\n",
    "1. Install Ollama on the Colab runtime\n",
    "2. Choose and download either 7B or 14B models\n",
    "3. Set up the environment for interactive use\n",
    "4. Provide examples and utilities for model interaction\n",
    "\n",
    "## üéØ Model Options:\n",
    "- **7B Models**: Perfect for Colab Free, faster responses, 4-6GB RAM\n",
    "- **14B Models**: Higher quality, requires Colab Pro, 8-12GB RAM\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è Important Notes:**\n",
    "- The model will run on Colab's servers, not your local machine\n",
    "- Free Colab has time limits; consider Colab Pro for longer sessions and 14B models\n",
    "- Models are downloaded fresh each session (not persistent)\n",
    "- 14B models may timeout on free Colab due to memory constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb702f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and system info\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "print(\"üîç System Information:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if GPU is available\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ GPU Available!\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"‚ùå No GPU detected\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå nvidia-smi not found - likely no GPU\")\n",
    "\n",
    "# Check system resources\n",
    "print(\"\\nüíæ System Resources:\")\n",
    "try:\n",
    "    result = subprocess.run(['free', '-h'], capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "except Exception as e:\n",
    "    print(f\"Could not get memory info: {e}\")\n",
    "\n",
    "print(\"\\nüíø Disk Space:\")\n",
    "try:\n",
    "    result = subprocess.run(['df', '-h', '/'], capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "except Exception as e:\n",
    "    print(f\"Could not get disk info: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17055c9d",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Ollama\n",
    "\n",
    "We'll download and install Ollama on the Colab runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b60d61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install Ollama\n",
    "print(\"üöÄ Installing Ollama...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Download and install Ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "print(\"\\n‚úÖ Ollama installation completed!\")\n",
    "\n",
    "# Verify installation\n",
    "!which ollama\n",
    "!ollama --version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb50e74",
   "metadata": {},
   "source": [
    "## üîÑ Step 2: Start Ollama Service\n",
    "\n",
    "We need to start the Ollama service in the background before we can use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0c929",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Start Ollama service in background\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"üîÑ Starting Ollama service...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def start_ollama_server():\n",
    "    \"\"\"Start Ollama server in background\"\"\"\n",
    "    try:\n",
    "        # Start ollama serve in background\n",
    "        process = subprocess.Popen(['ollama', 'serve'],\n",
    "                                 stdout=subprocess.PIPE,\n",
    "                                 stderr=subprocess.PIPE,\n",
    "                                 text=True)\n",
    "        return process\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting Ollama server: {e}\")\n",
    "        return None\n",
    "\n",
    "def start_server_background():\n",
    "    \"\"\"Start server in background thread\"\"\"\n",
    "    global server_process\n",
    "    server_process = start_ollama_server()\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_process = None\n",
    "server_thread = threading.Thread(target=start_server_background, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait a bit for the server to start\n",
    "print(\"‚è≥ Waiting for Ollama server to start...\")\n",
    "time.sleep(10)  # Increased wait time\n",
    "\n",
    "# Check if server is running\n",
    "def check_server():\n",
    "    try:\n",
    "        response = subprocess.run(['curl', '-s', 'http://localhost:11434/api/tags'],\n",
    "                                capture_output=True, text=True, timeout=10)\n",
    "        return response.returncode == 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if check_server():\n",
    "    print(\"‚úÖ Ollama server is running!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Server might not be ready yet. Trying alternative method...\")\n",
    "    # Alternative method - start directly\n",
    "    os.system('nohup ollama serve > /dev/null 2>&1 &')\n",
    "    time.sleep(5)\n",
    "    if check_server():\n",
    "        print(\"‚úÖ Ollama server is now running!\")\n",
    "    else:\n",
    "        print(\"‚ùå Server startup may need more time. Continue to next cell.\")\n",
    "\n",
    "print(\"\\nüéØ Server should be running on http://localhost:11434\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d083f",
   "metadata": {},
   "source": [
    "## üì• Step 3: Download and Load 7B Model or 14B\n",
    "\n",
    "Now we'll download a 7B parameter model. We'll use Llama 2 7B, but you can choose from other available models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c41cf4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Available models you can choose from:\n",
    "print(\"üéØ Available Models (7B and 14B):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 7B Models - Faster, less memory usage\n",
    "models_7b = {\n",
    "    \"llama2:7b\": \"Meta's Llama 2 7B - Good general purpose model\",\n",
    "    \"mistral:7b\": \"Mistral 7B - Efficient and fast model\",\n",
    "    \"codellama:7b\": \"Code Llama 7B - Specialized for code generation\",\n",
    "    \"vicuna:7b\": \"Vicuna 7B - Fine-tuned for conversations\",\n",
    "    \"orca-mini:7b\": \"Orca Mini 7B - Small but capable model\"\n",
    "}\n",
    "\n",
    "# 14B Models - Higher quality, more memory usage\n",
    "models_14b = {\n",
    "    \"llama2:13b\": \"Meta's Llama 2 13B - Higher quality general purpose\",\n",
    "    \"codellama:13b\": \"Code Llama 13B - Advanced code generation\",\n",
    "    \"vicuna:13b\": \"Vicuna 13B - Superior conversational abilities\",\n",
    "    \"wizard-vicuna-uncensored:13b\": \"Wizard Vicuna 13B - Uncensored responses\"\n",
    "}\n",
    "\n",
    "print(\"üöÄ 7B Models (Recommended for Colab Free):\")\n",
    "for model, description in models_7b.items():\n",
    "    print(f\"  ‚Ä¢ {model}: {description}\")\n",
    "\n",
    "print(f\"\\nüî• 13B-14B Models (Requires more memory - Colab Pro recommended):\")\n",
    "for model, description in models_14b.items():\n",
    "    print(f\"  ‚Ä¢ {model}: {description}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä Memory Requirements:\")\n",
    "print(\"  ‚Ä¢ 7B models: ~4-6GB RAM\")\n",
    "print(\"  ‚Ä¢ 13B models: ~8-12GB RAM\")\n",
    "print(\"  ‚Ä¢ GPU acceleration helps significantly with larger models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model selection with size consideration\n",
    "print(\"\\nüéØ Model Selection:\")\n",
    "use_14b = input(\"Do you want to use a 14B model? (y/n, default=n): \").lower().strip()\n",
    "\n",
    "if use_14b in ['y', 'yes']:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: 14B models require more memory!\")\n",
    "    print(\"   ‚Ä¢ Make sure you have GPU runtime enabled\")\n",
    "    print(\"   ‚Ä¢ Consider Colab Pro for better performance\")\n",
    "\n",
    "    model_choice = input(f\"\\nChoose 14B model (1-{len(models_14b)}, default=1): \").strip()\n",
    "    model_list = list(models_14b.keys())\n",
    "\n",
    "    try:\n",
    "        model_idx = int(model_choice) - 1 if model_choice else 0\n",
    "        model_name = model_list[model_idx]\n",
    "    except (ValueError, IndexError):\n",
    "        model_name = model_list[0]  # Default to first 14B model\n",
    "\n",
    "    print(f\"üì• Selected: {model_name}\")\n",
    "    download_time = \"10-20 minutes\"\n",
    "else:\n",
    "    print(\"\\n‚úÖ Using 7B model (recommended for Colab)\")\n",
    "    model_choice = input(f\"Choose 7B model (1-{len(models_7b)}, default=1): \").strip()\n",
    "    model_list = list(models_7b.keys())\n",
    "\n",
    "    try:\n",
    "        model_idx = int(model_choice) - 1 if model_choice else 0\n",
    "        model_name = model_list[model_idx]\n",
    "    except (ValueError, IndexError):\n",
    "        model_name = \"llama2:7b\"  # Default fallback\n",
    "\n",
    "    print(f\"üì• Selected: {model_name}\")\n",
    "    download_time = \"5-10 minutes\"\n",
    "\n",
    "print(f\"\\nüöÄ Pulling model: {model_name}\")\n",
    "print(f\"‚ö†Ô∏è  This may take {download_time} depending on your connection...\")\n",
    "\n",
    "# Pull the model\n",
    "# subprocess should already be imported from previous cells\n",
    "\n",
    "print(f\"‚¨¨ Downloading {model_name}...\")\n",
    "result = subprocess.run(['ollama', 'pull', model_name],\n",
    "                       capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"\\n‚úÖ Model {model_name} downloaded successfully!\")\n",
    "    print(f\"üíæ Model size: {model_name.split(':')[1].upper()}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Error downloading model:\")\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "    print(\"üí° Try restarting the Ollama service and try again\")\n",
    "    print(\"üí° For 14B models, ensure you have sufficient memory/GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628abdda",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## üß™ Step 4: Test the Model\n",
    "\n",
    "Let's run a simple test to make sure everything is working!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2fb213",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test the model with a simple prompt\n",
    "print(\"üß™ Testing the model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_prompt = \"hey hi tell me a joke\"\n",
    "\n",
    "print(f\"üìù Prompt: {test_prompt}\")\n",
    "print(\"\\nü§ñ Response:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Run the model\n",
    "!ollama run {model_name} \"{test_prompt}\"\n",
    "\n",
    "print(\"\\n‚úÖ Test completed! Your model is working!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
